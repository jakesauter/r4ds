---
title: "Modelling"
author: "Jake Sauter"
output: 
  html_document: 
    toc: true
    toc_float: true
    df_print: kable 
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  cache = FALSE, 
  warning = FALSE, 
  message = FALSE,
  hold = TRUE)

library(knitr)
library(kableExtra)

kable <- function(data, ...) {
  knitr::kable(data, digits=3, ...) %>% 
    kable_styling(bootstrap_options = c('striped', 
                                        'hover', 
                                        'responsive'),
                  full_width = FALSE, 
                  position = 'center', )
}

knit_print.data.frame <- function(x, ...) {
  res <- paste(c('', '', kable(head(x))), collapse = '\n')
  asis_output(res)
}

registerS3method('knit_print', 'data.frame', knit_print.data.frame)
```

# Modelling

```{r libraries}
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```


```{r}
models <- tibble(
  a1 = runif(250, -20, 40), 
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) + 
  geom_point()
```

```{r}
# Generate predictions given a model and data
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

```{r}
# Measure the loss of a model given the model and test data 
measure_distance <- function(model, data) {
  diff <- data$y - model1(model, data)
  sqrt(mean(diff^2)) # rms deviation
}

measure_distance(c(7, 1.5), sim1)
```

```{r}
# Now we can use purrr to compute the rms deviation
# of all of our models

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, ~measure_distance(c(..1, ..2), sim1)))

models
```


Now lets overlay 10 of the best models onto the data

```{r}
ggplot(sim1, aes(x,y)) + 
  geom_point(size = 2, color = 'grey30') + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```


Not too bad, lets take a look at the parameter space of these models
to gain some intuition about why thery are good

```{r}
ggplot(models, aes(a1, a2)) + 
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, color = 'red') + 
  geom_point(aes(color = -dist))
```


## Grid Search

So it seems like the parameters are all in a similiar neighborhood
in the parameter space, lets try to grid search this space 
to get a more optimal model

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length.out = 25), 
  a2 = seq(1, 3, length.out = 25)
) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, ~ measure_distance(c(..1, ..2), sim1)))

grid %>% 
  ggplot(aes(a1, a2)) + 
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, color = 'red') + 
  geom_point(aes(color = -dist))
```

Now lets see how these models do 

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = 'grey30') + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = dist), 
    data = filter(grid, rank(dist) <= 10)
    )
```

Much better! One can see how if we iterated on this idea that we would soon fine
the line of best fit. Fortunately we do not have to implement this ourselves as it 
is implemented multiple ways across R. 

## Model Optimization

We can provide `stats::optim()` an initial parameter value as well as a loss function 
and some data in order for it to search the parameter space in an intelligent way 
for us and return the optimal parameter settings!

We can provide some fancier parameters to this function such as `gr`, which is a function
that returns gradient values for the function at a point in parameter space, but
it works just fine for small models empirically. 

```{r}
best <- stats::optim(c(0,0), measure_distance, data = sim1)
best$par

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = 'gray30') + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```


As we can see this model fits the data nearly perfectly. 

Though because our model fits the general model family of models, we 
can use R's build in `base::lm()` function to find the optimal parameter
settings deterministically

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
best$par
```

















